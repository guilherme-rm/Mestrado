{  
   "//": "python main.py -c1 Config/config_1.json -c2 Config/config_2.json",
   "nagents": 30,
   "capacity": 500,
   "learning_rate": 0.01,
   "momentum": 0.005,
   "tau": 0.005,
   "batch_size": 8,
   "gamma": 0.9,
   
   "min_memory_for_learning": 8,
   "action_strategy": "boltzmann",
   
   "epsilon_policy": "linear_increasing",
   "eps_min": 0.0,
   "eps_max": 0.9,
   "eps_increment": 0.003,
   "eps_start": 1.0,
   "eps_end": 0.05,
   "eps_decay_steps": 50000,
   
   "nepisodes": 500,
   "nsteps": 50,
   "nupdate": 50,
   
   "early_termination": true,
   "cuda": 0,
   
   "enable_plot": true,
   "plot_interval": 50,
   "plot_smooth_window": 100,
   "plot_x_axis": "episodes",
   "step_log_throttle": 5,
   "checkpoint_interval": 100,
   
   "_doc": {
      "nagents": "Number of agents (UEs). Also sets DNN input size in RL.",
      "capacity": "Replay buffer capacity per agent (number of transitions).",
      "learning_rate": "Optimizer learning rate.",
      "momentum": "RMSprop momentum parameter (legacy).",
      "tau": "Soft target network update rate.",
      "min_memory_for_learning": "Minimum transitions before learning starts.",
      "action_strategy": "Action selection: epsilon_greedy, greedy, or boltzmann.",
      "epsilon_policy": "'linear_increasing' (original) or 'exponential_decay' (standard RL).",
      "eps_min": "Starting epsilon for linear_increasing policy.",
      "eps_max": "Maximum epsilon for linear_increasing policy.",
      "eps_increment": "Epsilon increment per step*episode for linear_increasing.",
      "eps_start": "Starting epsilon for exponential_decay policy.",
      "eps_end": "Final epsilon value for exponential_decay policy.",
      "eps_decay_steps": "Steps over which epsilon decays exponentially.",
      "batch_size": "Minibatch size sampled from replay for optimization.",
      "gamma": "Discount factor for target Q computation.",
      "nepisodes": "Number of training episodes.",
      "nsteps": "Max steps per episode.",
      "nupdate": "Legacy: target network hard update period.",
      "early_termination": "End episode early if all agents satisfy QoS.",
      "cuda": "GPU toggle (auto-detected if available).",
      "enable_plot": "Enable real-time training plot generation.",
      "plot_interval": "Render frequency (steps or episodes).",
      "plot_smooth_window": "Moving average window size for plots.",
      "plot_x_axis": "'steps' or 'episodes' for plot x-axis.",
      "step_log_throttle": "Write step_metrics.csv every N global steps.",
      "checkpoint_interval": "Save checkpoints every N episodes (0 disables)."
   }
}